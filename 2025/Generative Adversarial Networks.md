# GAN (Generative Adversarial Networks) 핵심 정리

## 1. 생성자와 판별자 (Concept)
GAN은 두 개의 모델이 서로 경쟁하면서 동시에 학습되는 구조이다. 둘의 관계는 **적대적 게임(Min-Max Game)**으로 요약할 수 있다.

> **핵심:** 생성자(G)는 판별자(D)를 속이려 하고, 판별자(D)는 속지 않으려 한다. 즉, **G는 D를 속이는 방향**으로, **D는 G를 잡아내는 방향**으로 서로 최적화가 진행된다.

### 용어 정의
* **Generative:** 생성자 G가 데이터를 생성한다.
* **Adversarial:** 판별자 D와 적대적 경쟁 관계를 가진다.
* **Networks:** 신경망 두 개(G와 D)가 함께 돌아가는 구조이다.

### 구조 상세
1.  **생성자 (Generator, G)**
    * **목표:** 가짜 데이터를 만들어내기 (판별자를 속이기 위함)
    * **입력:** 랜덤 노이즈 벡터 ($z$)
    * **출력:** 진짜처럼 보이는 가짜 이미지
2.  **판별자 (Discriminator, D)**
    * **목표:** 입력이 진짜인지 가짜인지 구분하기
    * **입력:** 이미지 (실제 데이터 $x$ 또는 G가 만든 가짜 $G(z)$)
    * **출력:** 진짜일 확률 (0 ~ 1 사이의 값)

---

## 2. GAN 학습 예시 (Analogy)
GAN은 **"위조지폐범(Counterfeiter) vs 경찰(Police)"**에 비유하면 가장 직관적이다.

* **G**: 위조지폐를 만드는 **위조범**
* **D**: 지폐가 진짜인지 검사하는 **경찰**

### 학습의 순환
1.  위조범($G$)은 더 정교한 지폐를 만든다.
2.  경찰($D$)은 위조를 더 잘 잡아낸다.
3.  이에 대응해 $G$는 더더욱 정교한 위조를 만든다.
4.  **결과:** 이 경쟁이 지속되면 결국 $G$는 **'D조차 구별 못 하는 수준의 진짜 같은 데이터'**를 만들게 된다.

---

## 3. GAN의 목적함수 (Minimax Objective)
GAN의 학습 목표는 다음의 수식 하나로 정리된다.

$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1 - D(G(z)))]$$

* **판별자(D)의 목표 (Maximize):**
    * 진짜 데이터 $x$에 대해서는 $D(x) \approx 1$
    * 가짜 데이터 $G(z)$에 대해서는 $D(G(z)) \approx 0$
    * 이 되도록 식의 값을 최대화한다.
* **생성자(G)의 목표 (Minimize):**
    * 가짜 데이터 $G(z)$를 D가 진짜라고 착각하게 만들어야 함 ($D(G(z)) \approx 1$)
    * 즉, $\log(1 - D(G(z)))$ 값을 최소화(음의 무한대 방향)하려 한다.

---

## 4. GAN의 장점
기존 생성 모델과 비교했을 때 GAN이 갖는 강점은 다음과 같다.

* **확률분포의 유연성:** 모델이 명시적인 확률분포(Explicit Density)를 가정하지 않는다.
* **복잡한 분포 학습:** 데이터 분포를 직접 추정할 필요 없이, 매우 복잡한 분포도 학습이 가능하다.
* **고품질 결과물:** 이미지 생성 및 변환 분야에서 큰 혁신을 가져왔으며, 결과물의 품질이 매우 높다.
* **효율성:** 샘플링 과정이 빠르고 자연스럽다.

---

## 5. GAN의 단점
GAN이 다루기 어렵다고 느껴지는 근본적인 이유는 **학습 안정성(Stability)** 문제 때문이다.

1.  **Mode Collapse (모드 붕괴)**
    * 생성자가 판별자를 속이기 쉬운 몇 개의 특정 패턴만 반복해서 생성하는 현상.
    * 결과물의 **다양성**이 사라진다.
2.  **학습 불안정 (Instability)**
    * $D$와 $G$의 균형(Balance)이 깨지면 학습이 진행되지 않는다.
    * $D$가 너무 강해도, $G$가 너무 강해도 문제가 발생한다.
3.  **Gradient Vanishing (기울기 소실)**
    * $D$가 초반에 너무 완벽하게 분류해버리면, $G$는 학습할 기울기(Gradient)를 전달받지 못해 학습이 멈춘다.

---

## 6. GAN의 발전 (Variants)
2014년 원본 논문 이후, 단점을 보완하기 위해 수많은 변종이 등장했다.

| 모델 이름 | 주요 특징 |
| :--- | :--- |
| **DCGAN** | CNN 구조를 적용한 GAN. 이미지 생성 GAN의 기본 골격이 됨. |
| **WGAN** | Wasserstein Distance(EMD)를 도입하여 학습 안정성을 극적으로 향상. |
| **WGAN-GP** | Gradient Penalty를 추가하여 WGAN을 더욱 안정화시킴. |
| **cGAN** | Conditional GAN. 클래스 정보(조건)를 넣어 특정 종류의 이미지를 생성 가능. |
| **CycleGAN** | 짝이 없는(Unpaired) 데이터셋 간의 스타일 변환 (예: 말 $\leftrightarrow$ 얼룩말). |
| **StyleGAN** | NVIDIA 개발. 고해상도 및 최고 수준의 얼굴 생성 능력. Latent space 제어 가능. |
| **BigGAN** | 대규모 데이터셋(ImageNet) 학습을 통해 고품질 이미지 생성 가능. |

---

## 7. 요약
> **"GAN의 생성자는 결국 데이터의 분포 $p_{data}(x)$를 학습하여, 그 분포에서 나올 법한 새로운 샘플을 생성하는 모델이다."**

즉, GAN의 최종 목표는 **기존 데이터와 구분할 수 없는 수준의 새로운 샘플을 만들 수 있는 분포를 학습**하는 것이며, 이를 위해 생성자와 판별자의 적대적 경쟁 구조를 활용한다.